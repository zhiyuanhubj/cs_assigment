{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5228 Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For code completion tasks, please write down your answer (i.e., your lines of code) between sentences that \"Your code starts here\" and \"Your code ends here\". For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed). For ease of checking your answers + grading, please keep your plain text answers in blue text.\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Save this Jupyter notebook as **A1_YourName_YourNUSNETID.ipynb** (e.g., **A1_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Submission deadline is 5 March 2023 (Sunday 11.59pm). You can submit it to Canvas under Assignments. You have 4 late days which can be used for either assignment, that extend the deadline by 24 hours each. No need to send any emails to use them, just submit late and they will be counted automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion.\n",
    "\n",
    "* **Q1: Data Cleaning & Exploratory Data Analysis (EDA) (30 Points)**\n",
    "    * 1 (a) Removing \"Dirty\" Records (6 Points)\n",
    "    * 1 (b) Handling Missing (NaN) Values (6 Points)\n",
    "    * 1 (c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "    * 1 (d) Handling of Categorical Attributes (4 Points)\n",
    "    * 1 (e) Basic Facts about a Real-World Dataset (8 Points)\n",
    "* **Q2: DBSCAN (10 Points)**\n",
    "    * 2 (a) Running DBSCAN and Visualization (3 Points)\n",
    "    * 2 (b) Effects of Data Manipulation on DBSCAN Results (3 Points)\n",
    "    * 2 (c) Identifying Noise/Outliers with Clustering beyond DBSCAN (4 Points)\n",
    "* **Q3: Clustering Algorithms (18 Points)**\n",
    "    * 3 (a) Questions about K-Means (12 Points)\n",
    "    * 3 (b) Interpreting Dendrograms (6 Points)\n",
    "* **Q4: Association Rule Mining (12 Points)**\n",
    "    * 4 (a) Compare the Runs A-D and Discuss your Observations! (4 Points) \n",
    "    * 4 (b) Compare the Runs A-D and discuss the results for building a recommendation engine! (4 Points)\n",
    "    * 4 (c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from efficient_apriori import apriori\n",
    "from src.utils import support, confidence, show_top_rules\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Data Cleaning & Explorative Data Analysis (EDA) (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following tasks, we consider a dataset containing information 20,000 past resale transactions of condo flats. Each record (i.e., data samples) consists of 12 attributes. The following **data description** list all attributes together with a brief description of each attribute's data type / domain:\n",
    "\n",
    "* **transaction_id**: Unique ID of the resale transactions; an 8-digit integer number uniquely assigned to each transaction.\n",
    "* **url**: Unique link to a website documenting this transaction as a string value.\n",
    "* **name**: The name of the condo as a string value (e.g., \"estella gardens\", \"eedon green\").\n",
    "* **type**: The type of condo as string value (e.g., \"condominium\", \"apartment\").\n",
    "* **postal_district**: The postal district the condo is located in as integer value; Singapore has 28 postal districts: 1, 2, ..., 28 (cf. [here](https://www.ura.gov.sg/realEstateIIWeb/resources/misc/list_of_postal_districts.htm)).\n",
    "* **subzone**: The subzone the condo is located in as a string value.\n",
    "* **planning_area**: The planning area the condo is located in as a string value.\n",
    "* **region**: The region the condo is located in as a string value.\n",
    "* **date_of_sale**: The date (month & year) of the transaction as a string value (e.g., \"mar-19\", \"oct-20\").\n",
    "* **area_sqft**: The size of the condo flat in square feet as a positive integer value.\n",
    "* **floor_level**: The range of floors in which the flat is located in the condo as string value (e.g., \"06 to 10\", \"11 to 15\").\n",
    "* **eco_category**: The eco category of the condo as a single-character string value (e.g., \"A\", \"B\", \"C\", \"D\").\n",
    "* **price**: Resale price of the condo flat in Singapore Dollar as an integer value.\n",
    "\n",
    "Additional information: Singapore has 55 planning areas; each split into multiple subzones (if interested, you can check the corresponding [Wikipedia article](https://en.wikipedia.org/wiki/List_of_places_in_Singapore)).\n",
    "\n",
    "**Important:** In each of the following subtasks we use a slightly different version of the dataset. This allows you to focus on the specific aspects of data cleaning / data preprocessing addresses in the respective subtask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (a) Removing \"Dirty\" Records (6 Points)\n",
    "\n",
    "We argued in the lecture that almost all real-world datasets contain some form of noise that might negatively affect any applied data analysis. The very first -- and in some sense -- easiest way to identify noise is to check if all data confirms with the data description. The following code cell shows a snippet of the dataset which you will be looking at in this subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_dirty = pd.read_csv('data/a1-condo-resale-dirty.csv')\n",
    "\n",
    "df_condo_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check the dataset against its description, you will notice that many records are \"dirty\". **We define a record as \"dirty\" if it does not adhere to the given data description (see above)**. Such records are not guaranteed to be valid and should therefore not be used for any analysis.\n",
    "\n",
    "**Identify 2 causes of \"dirty\" records and remove all corresponding records from the dataset!** Please provide your answer in the markdown cell below. Additional (simplifying) guidelines:\n",
    "\n",
    "* Ignore missing (`NaN`) values -- that is, a record containing one or more missing values does not make this record dirty. We look at missing values in a subsequent task.\n",
    "* Ignore the correctness of string values -- we do not expect you to check, e.g., if a street name contains a typo or a planning area is indeed one of the existing 55 planning areas in Singapore\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "(Your answer here)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for removing the \"dirty\" records. The results should back up your answer above.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- `pandas` is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points. But note that it's of course better to have a working solution using loops than having no solution at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_cleaned = df_condo_dirty.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After cleaning, there are now {} records.'.format(df_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after the cleaning step as there is some wiggle room regarding the performed steps which would affect this result. As such, even if two solutions are correct, they do not necessarily yield the same number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (b) Handling Missing (NaN) Values (6 Points)\n",
    "\n",
    "Many to most traditional data mining algorithms do not like missing (NaN) values and will throw an error if missing values are present. We therefore have to address missing values and get rid of them. On the other hand, we want to preserve as much of our dataset as possible, so we need to be smart about that. In this subtask, you are provided with a version of our condo resale dataset that contains missing values but is otherwise clean -- so it is all about the `NaN` values here.\n",
    "\n",
    "Let's load the dataset and have a quick look -- the attributes are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_nan = pd.read_csv('data/a1-condo-resale-nan.csv')\n",
    "\n",
    "df_condo_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your decision for handling `NaN` values might depend in the data mining task, assume in the following that you want to use this dataset to **create a regression model to predict the resale price** from the attributes of a transaction. Of course, there will be no need to actually create such a model here :).\n",
    "\n",
    "**Identify all `NaN` values in the dataset and handle them appropriately!** After this preprocessing, the resulting dataset should no longer contain any `NaN` values. Additional (simplifying) hints or guidelines:\n",
    "\n",
    "* You can use the `.info()` function of a pandas dataset to get info about its `NaN` values.\n",
    "* You do not need to consider external knowledge (i.e., information coming from outside this dataset), or sophisticated solutions such as [`sklearn.impute.KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). These can be very useful in practice (and maybe for your project), but their application requires certain assumptions to hold for good results. This is beyond the scope of this assignment.\n",
    "* Remove column `url`; one can argue that this is just a label and has no information for any analysis\n",
    "* Remove all records where `price` or `area_sqft` is `NaN`; these attributes are vey important for creating the model and there's no obvious way to reliably derive it\n",
    "* Derive missing `planning_area` values from subzone values; there's a clear mapping from a subzone to the corresponding planning area\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to actually implement your steps for handling `NaN` values. The results should back up your answer above.\n",
    "\n",
    "**Important:** Avoid using loops in the parts of the code you have to complete -- pandas is really powerful and should be your best friend here. If you use loops but the results are correct, there will be some minor deduction of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create a copy of the dataset and use this one to clean the data.\n",
    "df_no_nan = df_condo_nan.copy()\n",
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################\n",
    "\n",
    "print('After handling missing values, there are now {} records.'.format(df_no_nan.shape[0]))\n",
    "print('Number of records with an NaN for any attribute: {}'.format((df_no_nan.isna().sum(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after this preprocessing step as there is some wiggle room regarding the performed steps which would affect this result. However, the number of records with `NaN` values should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "\n",
    "Identifying \"dirty\" records and missing data are two very fundamental and generally rather systematic steps as part of data cleaning / data preprocessing. However, as we saw in the lecture using some examples, there are many other issues with the dataset that can be considered noise and thus potentially negatively affecting any data analysis. So the more noise we can remove, the more likely we can expect meaning analysis results.\n",
    "\n",
    "For this subtask, we use a version of our condo resale dataset **with no \"dirty\" records or missing data**! Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_others = pd.read_csv('data/a1-condo-resale-others.csv')\n",
    "\n",
    "df_condo_others.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your boss points out the following data quality issues; please explain what the issues are and how you would deal with them:** Please provide your answer in the markdown cell below list necessary steps with a justification for your decision. You may want to create a cell below to explore the data, which you can delete afterwards.\n",
    "\n",
    "* Outliers in `area_sqft`;\n",
    "* Inconsistent naming in `planning_area`;\n",
    "\n",
    "Additional (simplifying) guidelines:\n",
    "* You should still assume that we want to use this dataset to create a model for predicting the resale price of a flat based on its attributes. The choice of data mining task can affect your decision for what cleaning / preprocessing steps to apply.\n",
    "* There is no need to consider external knowledge. For example, you do not have to check if a value for `subzone` is indeed an existing subzone of Singapore.\n",
    "* There is no need for you to implement any processing steps! Most important are your justifications for your decisions.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "(Your answer here)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (d) Handling Categorical Attributes (4 Points)\n",
    "\n",
    "Many to most data mining algorithms require all input features / attributes to be numerical. Our dataset with transactions resales of condo flats contains attributes that are not all numerical. As such, assuming we indeed want to utilize them, we need to convert those attributes into numerical ones. Regarding encoding techniques, we covered [One-Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in the lecture. For variables with too many categories (e.g. >30) to be suitable for one-hot encoding, a common alternative is [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html), which replaces each category with a numeric value (roughly, the average value of the target variable for that particular category).\n",
    "\n",
    "Some common choices for how to handle a categorical attribute include\n",
    "* **Drop**: to drop a variable if it is not likely to be useful for modelling\n",
    "* **Ordinal**: treat it as an ordinal variable\n",
    "* **One-Hot Encoding**: encode each of its categories into a binary attribute\n",
    "* **Target Encoding**: encoder the entire variable into a single numerical attribute\n",
    "\n",
    "**For each of the 4 above choices, select *1* variable that you believe is suitable to be handled using that choice, and justify.**\n",
    "\n",
    "There is no single correct answer for this task; it's your justification that matters. Again, assume that we want to create a regression model to predict the resale price of a flat based on the other features.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "(Your answer here)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(e) Basic Facts about a Real-World Dataset (8 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the Condo Resale Prices dataset. As the data preprocessing steps you choose to perform might affect the results of this task, we will use a modified version here. Note that this version contains 20,000 listing of condo resale transactions and does **not** contain any \"dirty\" records. This is to ensure that everyone uses the same data. This helps marking your solutions as we know which results to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condo_facts = pd.read_csv('data/a1-condo-resale-facts.csv')\n",
    "\n",
    "df_condo_facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the table below by answering the given questions. Use the code cell below the table to actually implement your steps that enabled you to answer the questions. There is no need for a fancy layout for any print statement; it's only important that the result is clear.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)-(6). Answers (1)-(4) are worth 1 Point each; Answer (5)-(6) are worth 2 Points.\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| (1)  | What is the date (month & year) of the earliest transaction? | <font color='blue'>Your answer</font> |\n",
    "| (2)  | For each `type`, how many transactions are in the dataset?  | <font color='blue'>condominium: 9688, apartment: 8038, executive condominium: 2255, strata terrace: 14, strata semi-detached: 3, strata detached: 2</font> |\n",
    "| (3)  | What is the planning area with the most transactions? List the name of the planning area and the number of transactions!  | <font color='blue'>Name of the planning area: bedok, Transactions Count: 1307</font> |\n",
    "| (4)  | What is the correlation between the resale *price* and *area_sqft*? | <font color='blue'>Your answer</font> |\n",
    "| (5)  | Which transaction in postal district 11 had the highest price-to-area ratio (i.e., the highest price per square foot)? List the name of the condo and the price per square foot (rounded to 2 decimals)| <font color='blue'>pullman residences newton with a price per square foot of 3083.96</font> |\n",
    "| (6)  | What is the number of transactions where the apartment was between the 51st floor (inclusive) and 60th (inclusive) floor?  | <font color='blue'>23</font> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    
    (1)
    "(2) \n",
    "counts = df_condo_facts['type'].value_counts() \n",
    'print(f"Number of transaction for each type: \n) \n',
    '\n',
    
    "(3) \n", 
    "counts = df_condo_facts['planning_area'].value_counts().nlargest(1) \n",
    'print(f"Name of the planning area: {counts.index[0]}, Transactions Count: {counts.values[0]}") \n',
    "\n",
    
    "(4) \n",
    
    
    "(5) \n",
    "# Load the data from CSV file into a DataFrame \n",
    "df_condo_facts = pd.read_csv('data/a1-condo-resale-facts.csv') \n",

    "# Filter the data for postal district 11 \n",
    "df_condo_facts = df_condo_facts[df_condo_facts['postal_district'] == 11] \n",

     "# Calculate the price per square foot for each transaction \n",
     "df_condo_facts['price_per_sqft'] = df_condo_facts['price'] / df_condo_facts['area_sqft'] \n",

     "# Find the transaction with the highest price per square foot \n",
     "max_pps = df['price_per_sqft'].max() \n",
     "condo_name = df.loc[df['price_per_sqft'].idxmax(), 'name'] \n",

     "# Print the result \n",
     'print(f"The transaction with the highest price-to-area ratio in postal district 11 is {condo_name} with a price per square foot of {max_pps:.2f}.") \n',
    
    "\n",
    "(6) \n",
    "# filter transactions where the floor is between 51 and 60 (inclusive) \n",
    'filtered_df = df_condo_facts[(df_condo_facts["floor_level"] >= '51') & (df_condo_facts["floor_level"] <= '60')] \n',

    "# count the number of transactions \n",
    "num_transactions = len(filtered_df) \n",

    'print("Number of transactions with apartment between 51st and 60th floor (inclusive):", num_transactions) \n',
       
    "\n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: DBSCAN (10 Points)\n",
    "\n",
    "In Section 1 we focused on addressing any \"obvious\" noise a given dataset may contain. Obvious noise is not well defined, but in general it's this kind of noise that can be identified by simple analysis (e.g., looking at the domains of attributes, or simple statistics such as the distribution/histogram). However, some noise is more difficult to detect. Some outliers can only be identified as such when looking at the combination of different data points. Looking at individual attributes / features does not suffice here.\n",
    "\n",
    "An outlier refers to some record (i.e., data samples) that is very different compared to most other records. Next we see how we can utilize DBSCAN for this task. DBSCAN is a clustering algorithm with an explicit notion of noise points, i.e., data points that are dissimilar to data points that form clusters.\n",
    "\n",
    "We use in this section a small toy dataset -- 70 records, 2 numeric attributes -- for easy testing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a1-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a) Running DBSCAN and Visualization (3 Points)\n",
    "\n",
    "**Run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset**. Use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters.\n",
    "\n",
    "The scikit-learn output contains a `labels_` variable, where, noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise.\n",
    "\n",
    "Your output of running `DBSCAN(...)` should be saved as a variable called `dbscan_clustering`, and you should construct a binary vector (of length 70) called `is_noise`, where `True` indicates noise points. (As long as the subsequent visualization works, it indicates that you have done it correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################################\n",
    "### Your code starts here ###############################################################\n",
    "\n",
    "# Run DBSCAN(...); the output should be saved in a variable called `dbscan_clustering`, \n",
    "# and also construct a binary variable called `is_noise`, where True indicates noise points.\n",
    "\n",
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Perform DBSCAN \n",
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=10).fit(X_dbscan_toy) \n",
    "\n",
    "# Identify noise points \n",
    "is_noise = np.zeros_like(dbscan_clustering.labels_, dtype=bool) \n",
    "is_noise[dbscan_clustering.labels_ == -1] = True \n",
    "### Your code ends here #################################################################\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have done it correctly, the following code should successfully plot the clusters and outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise points plotted as black crosses; other points plotted as circles with color indicating cluster\n",
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[~is_noise,0], X_dbscan_toy[~is_noise,1], c=dbscan_clustering.labels_[~is_noise], marker='o')\n",
    "plt.scatter(X_dbscan_toy[is_noise,0], X_dbscan_toy[is_noise,1], c='black', marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (b) Effects of Data Manipulation on DBSCAN Results (3 Points)\n",
    "\n",
    "Assume you have a $d$-dimensional dataset `X` in the Euclidean space, i.e., each data point as $d$ numerical features (with each feature value in the interval $[0, 1]$). After running DBSCAN over `X`, you get some clustering (again, we only assume it's not only noise). Now you create a new dataset `X_new` by multiplying all data points by 10 afterwards adding 100 to all data points (in Python, assuming X is a NumPy array this can simply be done by `X_new = X * 10 + 100`). Now you can run DBSCAN over `X_new`.\n",
    "\n",
    "**Explain how you have to change the parameters of DBSCAN for `X_new` to produce equivalent output to the original results on `X`!**. You can ignore any nondeterminism (e.g. border points being assigned to different clusters, as discussed in lecture), or duplicates.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "To obtain equivalent clustering results on the scaled dataset X_new, we need to scale the epsilon and minimum number of points parameters by a factor of 10, i.e., eps_Xnew = 10 * eps_X and minPts_Xnew = 10 * minPts_X. This is because the distance between any two points is scaled by a factor of 10 after scaling and shifting the data points in X_new. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (c) Identifying Noise/Outliers with Clustering beyond DBSCAN (4 Points)\n",
    "\n",
    "Apart from DBSCAN, we also covered two other important clustering algorithms: K-Means and (agglomerative) hierarchical clustering. For all three clustering algorithms we looked in detail into their approach, and also discussed the individual strengths, weaknesses, and limitations. Particularly we saw that of these three algorithms, only DBSCAN has this explicit notion of noise points. But what about K-Means and hierarchical clustering?\n",
    "\n",
    "**Explain if K-Means and/or hierarchical clustering can potentially be utilized to identify noise/outliers in a dataset!** If your answer for an algorithm is \"No\", please provide a brief justification. If your answer for an algorithm is \"Yes\", provide a brief sketch (no pseudo code required; a basic description will do) how to use the algorithm for noise/outlier detection.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "K-Means and hierarchical clustering algorithms can be utilized for outlier detection, but they are not as effective as DBSCAN, which has an explicit notion of noise points and can accurately identify outliers in a dataset. \n",
    "\n",
    "In K-Means clustering, an outlier point can be identified as a data point that does not belong to any cluster or is far away from the centroids of all clusters. To identify outliers in K-Means clustering, we can apply a threshold distance from the closest centroid or a minimum number of points required to form a cluster. Any data point that does not meet these criteria can be considered an outlier. However, this approach is not very reliable as the definition of an outlier is subjective and dependent on the distance metric used. \n",
    "\n",
    "In hierarchical clustering, an outlier point can be identified as a data point that is far away from all other points in the dataset or does not belong to any cluster. To identify outliers in hierarchical clustering, we can inspect the dendrogram and look for data points that form a single point or a small cluster at a large distance from all other points or clusters. These points can be considered outliers. However, this approach can be subjective and dependent on the distance metric and the linkage method used. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Clustering Algorithms (18 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (a) Questions about K-Means (12 Points)\n",
    "\n",
    "In the table below are 6 statements that are either True or False. Complete the table to specify whether a statement is True or False, and provide a brief explanation for your answer (Your explanation is more important than a simple True/False answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)~(6).\n",
    "\n",
    "| No. | Statement                                                                                                   | True or False?       | Brief Explanation |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| (1)  | When using K-Means with K-Means++, then centroids are at all times at the position of existing data points | <font color=\"blue\">F</font> | <font color=\"blue\">K-Means++ selects initial centroids randomly and subsequent centroids based on the distance from existing centroids, not data points themselves.Additionally, centroids move to the center of their respective clusters during iterative refinement, which may not coincide with any existing data point.</font> \n",
    "| (2)  | K-Means++ ensures that the result will not include any empty clusters. | <font color=\"blue\">T</font> | <font color=\"blue\">K-Means++ ensures that the result will not include any empty clusters because it guarantees that each centroid is initialized with at least one data point, and subsequent centroids are chosen with a probability proportional to the squared distance from the nearest existing centroid. This ensures that each cluster will have at least one point assigned to it, which prevents empty clusters.</font> \n",
    "| (3)  | K-Means, independent of the initialization method, will always converge to a local minimum (note: the global minimum is also counted as a local minimum) | <font color=\"blue\">T</font> | <font color=\"blue\">K-Means, independent of the initialization method, will always converge to a local minimum, including the global minimum. This is because the algorithm iteratively minimizes the sum of squared distances between data points and their assigned centroids, and this objective function is guaranteed to converge to a local minimum. However, K-Means does not guarantee that it will converge to the global minimum, as the algorithm can get stuck in a suboptimal solution based on the initial centroid positions.</font> \n",
    "| (4)  | K-Means++ will always converge to the global optimum. | <font color=\"blue\">F</font> | <font color=\"blue\">K-Means++ does not guarantee that it will always converge to the global optimum, as it is still a heuristic method and can still get stuck in a suboptimal solution based on the initial centroid positions. However, K-Means++ is more likely to converge to a better solution compared to random initialization, as it selects the initial centroids in a way that encourages better cluster separation and reduces the chances of getting stuck in a suboptimal solution.</font> \n",
    "| (5)  | K-Means++ initialization is more costly than a random initialization of the centroids but generally converges faster. | <font color=\"blue\">T</font> | <font color=\"blue\">K-Means++ initialization is more costly than random initialization of the centroids, as it involves an additional step of selecting initial centroids with a probability proportional to the squared distance from the nearest existing centroid. However, K-Means++ generally converges faster than random initialization, as the better initialization of centroids reduces the number of iterations required for convergence. In practice, the computational cost of K-Means++ initialization is often negligible compared to the cost of the iterative refinement process, and the improved convergence speed justifies the additional initialization cost.</font> \n",
    "| (6)  | K-Means is insensitive to data normalization/standardization -- that is, for the same $k$ and the same initial centroids, K-Means will yield the same clusters where the data is normalized/standardized or not. | <font color=\"blue\">F</font> | <font color=\"blue\">Data normalization/standardization scales the values of the data to a common range, which can have a significant impact on the distance metric used by K-Means to calculate the similarity between data points. If the data is not normalized/standardized, features with larger scales will dominate the distance calculations and bias the clustering towards those features. Therefore, it is generally recommended to normalize/standardize the data before applying K-Means to ensure that all features contribute equally to the distance calculations and to obtain consistent results across different scaling ranges of the data.</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 (b) Interpreting Dendrograms (6 Points)\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering. Properly interpreting is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-agnes-data-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 6 dendrograms labeled 1-6. These dendograms show the clustering using **(Agglomerative) Hierarchical Clustering with Single Linkage** for the 6 datasets above, but in a random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/a1-agnes-dendrogram-labeled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the correct combinations of datasets and dendrograms** -- that is, find for each dataset the corresponding dendrogram! Give a brief explanation for each decision and complete the table below.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |\n",
    "| **B**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |\n",
    "| **C**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |\n",
    "| **D**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |\n",
    "| **E**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |\n",
    "| **F**    | <font color=\"blue\">?</font> | <font color=\"blue\">Explanation here.</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Association Rule Mining (12 Points)\n",
    "\n",
    "Next, we focus on the **Apriori Algorithm for finding Frequent Itemsets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset has 5 transactions and 6 different items. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of running the `efficient-apriori` package  (nothing for you to do here!)\n",
    "\n",
    "We run the apriori algorithm over the demo data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.4, min_confidence=1.0)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommending Movies using Association Rule Mining (ARM)\n",
    "\n",
    "In this task, we look into using Association Rule Mining for recommending movies -- more specifically, recommending movies on physical mediums (Blu-ray, DVD, etc.), assuming that is still a thing nowadays :).\n",
    "\n",
    "**Dataset.** We use a popular movie ratings dataset from [MovieLens](https://grouplens.org/datasets/movielens/). This dataset contains user ratings for movies (1-5 stars, incl. half stars, e.g., 3.5). Specifically, we use the [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/) containing 1 Million ratings from ~6,000 users on ~4,000 movies and was released February 2003 -- so do not expect any recent Marvel movies :).\n",
    "\n",
    "While there are more sophisticated recommendation algorithms -- and we will look into those in a later lecture -- here we focus on Association Rules. We convert this rating dataset into a transaction dataset, where a transaction represents all the movies a user has purchased. We already did this for you making the following assumption: A User has purchased all the movies s/he gave the highest rating. For example, if User A gave a highest rating of 4.5 to any movie, A has purchased all movies A rated with 4.5. This is certainly a simplifying assumption, but perfectly fine for this task here.\n",
    "\n",
    "Let's have a quick look at the data. First, we load the ids and names of all movies into a dictionary. We need this dictionary since our transactions (i.e., the list of movies a user has bought) contains the ids and not the names of the movies. So to actually see the names of movies in the association rules, we need this way to map from a movie's id to its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file with movies (and ids) into a pandas dataframe\n",
    "df_movies = pd.read_csv('data/a1-arm-movies.csv', header=None)\n",
    "# Convert dataframe to dictionary for quick lookups\n",
    "movie_map = dict(zip(df_movies[0], df_movies[1]))\n",
    "# Show the first 5 entries as example\n",
    "for movie_id, movie_name in movie_map.items():\n",
    "    print('{} -> {}'.format(movie_id, movie_name))\n",
    "    if movie_id >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we can load the transactions. Again, a transaction is a user's shopping history, i.e., all the movies the user has bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping_histories = []\n",
    "\n",
    "# Read shopping histories; each line is a comma-separated list of the movies (i.e., their ids!) a user bought\n",
    "with open('data/a1-arm-movie-shopping-histories.csv') as file:\n",
    "    for line in file:\n",
    "        shopping_histories.append(tuple([ int(i) for i in line.strip().split(',') ]))\n",
    "\n",
    "# Show the shopping history of the first user for an example; we need movie_map to get the name of each movie\n",
    "user = 0\n",
    "\n",
    "print('Shopping history for user {} (used for Apriori algorithm)'.format(user))\n",
    "print(shopping_histories[user])\n",
    "print()\n",
    "print('Detailed shopping history for user {}'.format(user))\n",
    "for movie_id in shopping_histories[user]:\n",
    "    print('{}: {}'.format(movie_id, movie_map[movie_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset loaded, we are ready to find interesting Association Rules. For performance reasons, we use the `efficient_apriori` package.\n",
    "\n",
    "For added convenience, we provide method `show_top_rules()` which computes the Association Rules using the `efficient-apriori` package, but (a) sorts the rules w.r.t. the specified metric (default: lift), and (b) shows only the top-k rules (default: 5). The method also ensures a consistent output of each Association Rule. Each rule contains the LHS (left-hand side), RHS, as well as the support (s), confidence (c), and lift (l). Feel free to check out the code of method `show_top_rules()` in `src.utils` if anything might be unclear regarding its use.\n",
    "\n",
    "**Run the following 4 code cells!** All 4 code cells find Association Rules using the `efficient-apriori` package encapsulated in the auxiliary method `show_top_rules()` for convenience. Also, note that we call `show_top_rules()` with `id_map=None` at first, so the results will only display the movie ids. Later, you will be asked to run the cells again with `id_map=movie_map` to see the actual names of the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.2, k=10, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.2, k=10, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.15, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "# show_top_rules(shopping_histories, min_support=0.08, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 (a) Compare the Runs A-D and Discuss your Observations! (4 Points)\n",
    "\n",
    "You must have noticed numerous differences between the 4 runs A-D. List at least 2 differences you have found. You may want to consider the elapsed time and the resulting association rules. Briefly explain your observations! For this subtask, you do not need to look at the movie names (`id_map=None`) as your observations are not specific to the context of movie recommendations.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    Your answer here\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 (b) Compare the Runs A-D and discuss the results for building a recommendation engine! (4 Points)\n",
    "\n",
    "Now run the code cells above for Runs A-B again, but this time with `id_map=movie_map` so that the output will show for each rule the actual movie names.\n",
    "\n",
    "Comparing the results of the different runs again, but now seeing the actual movie names, should give you some further insights: particularly on (i) what you notice about the movie titles that appear on the LHS and RHS; and (ii) how the value of `min_support` affects the set of movies that appear. Explain the implications of your findings for building a recommendation engine.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    Your answer here\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 (c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)\n",
    "\n",
    "So far, we only looked at individual rules and how the set of rules changes for different parameter values for `min_support` and `min_confidence`. However, we still need some method like `make_recommendation(shopping_history)` that takes the shopping history of a user and returns 1 or more recommendations. You do *not* have to implement or provide code for such a method; just briefly sketch any reasonable way to do it, taking into account how to handle situations involving new users (empty shopping history); or users with long shopping history (more than the length of the left side of all our association rules).\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    Your answer here\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
